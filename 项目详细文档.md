# AI 五子棋 - 技术文档

## 目录

- [项目概述](#项目概述)
- [核心架构](#核心架构)
- [模块详解](#模块详解)
- [训练流程](#训练流程)
- [性能优化](#性能优化)
- [开发指南](#开发指南)

---

## 项目概述

这是一个基于深度强化学习的五子棋 AI 项目,使用策略-价值网络结合蒙特卡洛树搜索(MCTS)实现。通过自我对弈不断提升棋力。

### 技术栈

- **语言**: Python 3.6+
- **核心算法**: MCTS + 深度神经网络
- **深度学习框架**:
  - Theano + Lasagne (默认)
  - PyTorch (推荐)
  - TensorFlow
  - Keras
- **GUI**: Pygame
- **推理**: 纯 NumPy (无需框架依赖)

### 特色功能

- 多框架支持,接口统一
- 数据增强(旋转+翻转 8 倍)
- 自适应学习率
- MCTS 树复用
- 纯 NumPy 推理引擎

---

## 核心架构

### 系统分层

```
┌─────────────────────────────────────────────────┐
│            用户界面层                            │
│  main.py (GUI)  │  trainer.py (训练)            │
└──────────────────┬──────────────────────────────┘
                   │
┌──────────────────┴──────────────────────────────┐
│            游戏引擎层                            │
│  board.py - GameState / GameController         │
└──────────────────┬──────────────────────────────┘
                   │
┌──────────────────┴──────────────────────────────┐
│            AI 决策层                             │
│  neural_search.py  │  random_search.py         │
│  (神经网络 MCTS)    │  (纯 MCTS 基准)           │
└──────────────────┬──────────────────────────────┘
                   │
┌──────────────────┴──────────────────────────────┐
│          神经网络层                              │
│  model_*.py (多框架实现)                        │
│  model_inference.py (纯 NumPy 推理)             │
└─────────────────────────────────────────────────┘
```

### 数据流

#### 训练阶段
```
自我对弈 → 数据增强(8x) → 经验回放缓冲区 → 采样训练 → 评估 → 保存模型
```

#### 推理阶段
```
棋局状态 → 神经网络 → (策略分布, 局面价值) → MCTS 搜索 → 最佳落子
```

---

## 模块详解

### 1. board.py - 游戏引擎

#### GameState 类

**职责**: 维护棋盘状态,处理落子和规则判定

**核心数据结构**:
```python
self.positions: dict[int, int]      # {位置索引: 玩家ID}
self.openPositions: list[int]       # 可落子位置
self.activePlayer: int              # 当前执棋方 (1或2)
self.winCondition: int              # 获胜连珠数
```

**关键方法**:

```python
def getStateArray(self) -> np.ndarray:
    """
    返回 4 通道状态数组 (4, rows, cols):
    - 通道 0: 当前玩家棋子位置
    - 通道 1: 对手玩家棋子位置
    - 通道 2: 上一步落子位置
    - 通道 3: 先手标识 (先手全1, 后手全0)
    """
```

**索引转换**:
```python
def indexToCoord(self, idx):
    """一维索引 → (行, 列)"""
    return [idx // self.cols, idx % self.cols]

def coordToIndex(self, coord):
    """(行, 列) → 一维索引"""
    return coord[0] * self.cols + coord[1]
```

**胜负判定**:
```python
def checkVictory(self):
    """检查横/竖/斜四个方向是否有玩家获胜"""
    # 遍历所有已落子位置,检查连珠
    # 返回 (是否获胜, 胜者ID)
```

#### GameController 类

**职责**: 游戏流程控制,自我对弈数据生成

```python
def runSelfPlay(self, agent, temperature=1e-3):
    """
    自我对弈生成训练数据

    返回:
        - winner: 胜者ID
        - data: (state, mcts_probs, reward) 元组序列

    流程:
        1. AI 决策 + 记录 MCTS 概率分布
        2. 执行落子
        3. 游戏结束后分配奖励 (+1/-1)
    """
```

---

### 2. neural_search.py - 神经网络引导的 MCTS

#### SearchNode 类

**UCB 公式**:
```python
def computeScore(self, explorationWeight):
    """
    UCB = Q + c_puct * P * sqrt(N_parent) / (1 + N)

    Q: 节点平均价值
    P: 先验概率 (策略网络输出)
    c_puct: 探索系数 (默认 5)
    N: 节点访问次数
    """
    self._ucbBonus = (explorationWeight * self._priorProb *
                      np.sqrt(self._parentNode._visitCount) /
                      (1 + self._visitCount))
    return self._qValue + self._ucbBonus
```

**增量统计更新**:
```python
def updateStats(self, leafValue):
    """
    增量更新 Q 值: Q_new = Q_old + (V - Q_old) / N
    避免存储所有访问记录,节省内存
    """
    self._visitCount += 1
    self._qValue += (leafValue - self._qValue) / self._visitCount
```

**回溯更新**:
```python
def backpropagate(self, leafValue):
    """沿路径向上更新所有节点统计"""
    self.updateStats(leafValue)
    if self._parentNode:
        self._parentNode.backpropagate(-leafValue)  # 切换视角
```

#### MonteCarloTreeSearch 类

**单次模拟**:
```python
def _runSimulation(self, gameState):
    """
    MCTS 四步流程:

    1. Selection (选择):
       从根节点选择 UCB 最大的子节点,直到叶节点

    2. Expansion (扩展):
       用策略网络扩展叶节点的所有合法动作

    3. Evaluation (评估):
       用价值网络评估叶节点局面

    4. Backpropagation (回溯):
       反向传播评估值,更新路径上所有节点
    """
```

**树复用**:
```python
def advanceTree(self, lastAction):
    """
    将对应子节点提升为新根节点
    保留子树信息,避免重新搜索
    """
    if lastAction in self._rootNode._childNodes:
        self._rootNode = self._rootNode._childNodes[lastAction]
        self._rootNode._parentNode = None
```

#### TreeSearchAgent 类

**温度参数**:
```python
def selectMove(self, gameState, temperature=1e-3):
    """
    temperature 控制探索:
    - τ → 0: 选择访问次数最多的动作 (确定性)
    - τ = 1: 按访问次数比例随机选择 (探索性)
    """
```

**Dirichlet 噪声**(自我对弈模式):
```python
# 在根节点添加噪声增强探索
p = 0.75 * probs + 0.25 * np.random.dirichlet(0.3 * np.ones(len(probs)))
```

---

### 3. model_*.py - 神经网络实现

#### 网络架构

```
输入: (4, H, W) 状态数组
    │
    ▼
┌──────────────────┐
│ Conv1: 4→32      │  3×3, padding=1, ReLU
│ Conv2: 32→64     │  3×3, padding=1, ReLU
│ Conv3: 64→128    │  3×3, padding=1, ReLU
└────┬─────────────┘
     │
     ├───────────────────┐
     │                   │
     ▼                   ▼
┌────────────┐     ┌─────────────┐
│  策略头     │     │  价值头      │
│ Conv: 128→4│     │ Conv: 128→2 │
│ FC: 4HW→HW │     │ FC1: 2HW→64 │
│ Softmax    │     │ FC2: 64→1   │
│            │     │ Tanh        │
└────────────┘     └─────────────┘
     │                   │
     ▼                   ▼
动作概率分布        局面价值
P(a|s) ∈ [0,1]^HW  V(s) ∈ [-1,1]
```

#### 损失函数

```python
总损失 = 价值损失 + 策略损失 + L2 正则化

L = (z - v)² - π^T log(p) + c||θ||²

其中:
- z: 实际游戏结果 (+1=胜, -1=负)
- v: 价值网络预测
- π: MCTS 搜索得到的改进策略
- p: 策略网络输出
- c: L2 正则化系数
```

#### NeuralNetworkEvaluator 接口

所有框架实现遵循统一接口:

```python
class NeuralNetworkEvaluator:
    def evaluatePosition(self, gameState):
        """
        评估单个局面
        返回: (actionProbs, value)
        """

    def trainOnBatch(self, stateBatch, mctsProbs, winners, learningRate):
        """
        训练一个批次
        返回: (loss, entropy)
        """

    def saveCheckpoint(self, filePath):
        """保存模型参数"""
```

---

### 4. model_inference.py - NumPy 推理引擎

**功能**: 纯 NumPy 实现神经网络前向传播,无需深度学习框架

**核心函数**:

```python
def convolutionForward(input, weights, bias, padding, stride):
    """卷积层前向传播"""
    # 使用 im2col 加速卷积计算

def fullyConnectedForward(input, weights, bias):
    """全连接层前向传播"""

def applyRelu(x):
    """ReLU 激活"""
    return np.maximum(0, x)

def computeSoftmax(x):
    """Softmax 归一化"""
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()
```

**加载模型**:
```python
# 加载 Theano 训练的模型权重
modelParams = pickle.load(open('8_8_5.model', 'rb'))
network = NumpyNetworkEvaluator(8, 8, modelParams)

# 无需安装 Theano,直接推理
actionProbs, value = network.evaluatePosition(gameState)
```

---

### 5. trainer.py - 训练管道

#### 超参数配置

```python
class TrainingManager:
    def __init__(self):
        # 棋盘配置
        self.boardCols = 6
        self.boardRows = 6
        self.winLength = 4

        # MCTS 参数
        self.numSimulations = 400       # 每步模拟次数
        self.explorationWeight = 5      # UCB 探索系数

        # 训练参数
        self.miniBatchSize = 512        # 批次大小
        self.replayBufferSize = 10000   # 经验回放容量
        self.baseLearningRate = 2e-3    # 基础学习率
        self.klTarget = 0.02            # KL 散度目标

        # 评估参数
        self.evaluationInterval = 50    # 评估间隔
        self.totalBatches = 1500        # 总训练轮数
```

#### 数据增强

```python
def augmentData(self, gameData):
    """
    对称增强: 利用棋盘旋转和翻转对称性

    原始样本 → 8 个等价样本:
    - 原始
    - 旋转 90°, 180°, 270°
    - 上述 4 种的水平翻转

    注意: 状态和概率分布必须同步变换
    """
    for state, probs, winner in gameData:
        probsMatrix = probs.reshape(self.boardRows, self.boardCols)

        for i in [1, 2, 3, 4]:
            # 旋转
            rotState = np.array([np.rot90(s, i) for s in state])
            rotProbs = np.rot90(probsMatrix, i)
            yield (rotState, rotProbs.flatten(), winner)

            # 翻转
            flipState = np.array([np.fliplr(s) for s in rotState])
            flipProbs = np.fliplr(rotProbs)
            yield (flipState, flipProbs.flatten(), winner)
```

#### 自适应学习率

```python
def updatePolicy(self):
    """根据 KL 散度自适应调整学习率"""
    # 训练一个批次
    oldProbs = self.neuralNetwork.batchEvaluate(states)[0]
    loss, entropy = self.neuralNetwork.trainOnBatch(
        states, mctsProbs, winners,
        self.baseLearningRate * self.learningRateScale
    )
    newProbs = self.neuralNetwork.batchEvaluate(states)[0]

    # 计算 KL 散度
    klDivergence = np.mean(
        np.sum(oldProbs * (np.log(oldProbs + 1e-10) -
                           np.log(newProbs + 1e-10)), axis=1)
    )

    # 调整学习率
    if klDivergence > self.klTarget * 2:
        self.learningRateScale /= 1.5  # KL 过大,降低学习率
    elif klDivergence < self.klTarget / 2:
        self.learningRateScale *= 1.5  # KL 过小,提高学习率
```

#### 训练循环

```python
def runTraining(self):
    for i in range(self.totalBatches):
        # 1. 自我对弈
        self.generateSelfPlayData(1)

        # 2. 训练
        if len(self.replayBuffer) > self.miniBatchSize:
            loss, entropy = self.updatePolicy()

        # 3. 定期评估
        if (i + 1) % self.evaluationInterval == 0:
            winRatio = self.evaluatePolicy(numGames=10)

            # 胜率超 55%,保存为最佳模型
            if winRatio > 0.55:
                self.neuralNetwork.saveCheckpoint('./best_policy.model')

        # 4. 保存当前模型
        self.neuralNetwork.saveCheckpoint('./current_policy.model')
```

---

### 6. main.py - Pygame 图形界面

#### GomokuInterface 类

**初始化**:
```python
def __init__(self):
    # 棋盘配置
    self.BOARD_CONFIGS = [
        {"cols": 6, "rows": 6, "win": 4, "label": "6x6 (4子连珠)"},
        {"cols": 8, "rows": 8, "win": 5, "label": "8x8 (5子连珠)"},
    ]

    # 显示参数
    self.cellSize = 60
    self.margin = 40

    # 线程锁保护共享状态
    self.stateLock = threading.Lock()
```

**AI 加载**:
```python
def _loadAI(self):
    """加载对应棋盘的 AI 模型"""
    modelFile = f'{self.boardCols}_{self.boardRows}_{self.winLength}.model'

    with open(modelFile, 'rb') as f:
        modelParams = pickle.load(f)

    network = NumpyNetworkEvaluator(self.boardCols, self.boardRows, modelParams)
    self.aiAgent = TreeSearchAgent(
        network.evaluatePosition,
        explorationWeight=5,
        numSimulations=400
    )
```

**线程安全的落子**:
```python
def _handleHumanMove(self, x, y):
    """处理玩家落子"""
    moveIdx = self.gameState.coordToIndex([y, x])

    # 使用锁保护共享状态
    with self.stateLock:
        self.gameState.applyMove(moveIdx)
        self.lastMoveIdx = moveIdx

    # 启动 AI 思考线程
    threading.Thread(target=self._executeAiMove, daemon=True).start()

def _executeAiMove(self):
    """AI 落子 (在后台线程执行)"""
    with self.stateLock:
        moveIdx = self.aiAgent.selectMove(self.gameState)
        self.gameState.applyMove(moveIdx)
```

---

## 训练流程

### 完整训练周期

```
1. 初始化
   - 创建随机初始化的神经网络
   - 初始化空的经验回放缓冲区

2. 自我对弈 (每轮)
   for 每一步:
     - MCTS 搜索 (400 次模拟)
     - 根据访问次数选择落子
     - 记录 (状态, MCTS 概率分布)

   游戏结束:
     - 为每个状态分配奖励 (胜+1/负-1)
     - 生成训练样本 (state, π, z)

3. 数据增强
   - 旋转+翻转,扩充 8 倍
   - 存入经验回放缓冲区 (FIFO)

4. 策略更新
   - 从缓冲区随机采样 512 个样本
   - 计算损失并梯度下降
   - 根据 KL 散度调整学习率

5. 模型评估 (每 50 轮)
   - 新模型 vs 纯 MCTS (10 局)
   - 胜率 > 55% → 保存为最佳模型

6. 重复 1500 轮
```

### 为什么有效?

1. **自我提升循环**: AI 与自己对弈 → 发现新策略 → 训练网络 → AI 更强 → 循环

2. **MCTS 作为教师**: MCTS 搜索生成比网络本身更强的策略,网络学习模仿这个改进策略

3. **价值网络加速**: 直接评估局面,替代传统 MCTS 的随机模拟到终局

4. **策略网络指导**: 减少搜索宽度,集中算力在有希望的动作上

---

## 性能优化

### 训练加速

#### 1. 使用 GPU

```python
# model_torch.py
class NeuralNetworkEvaluator:
    def __init__(self, boardCols, boardRows, useGpu=True):
        self.useGpu = useGpu and torch.cuda.is_available()
        if self.useGpu:
            self.network = ConvolutionalNetwork(boardCols, boardRows).cuda()
```

预期加速: 10-50 倍

#### 2. 批量评估

```python
# 并行生成多局游戏数据
from multiprocessing import Pool

with Pool(processes=4) as pool:
    results = pool.map(self._singleGame, range(numGames))
```

#### 3. 调整 MCTS 参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| `numSimulations` | 训练: 200-400<br>对战: 800-1600 | 更多模拟更强但更慢 |
| `replayBufferSize` | 5000-20000 | 太小欠拟合,太大过拟合旧策略 |
| `miniBatchSize` | GPU: 512-1024<br>CPU: 128-256 | 受显存限制 |

### 推理优化

#### 1. 模型量化 (PyTorch)

```python
# 训练后量化
model_fp32.eval()
model_int8 = torch.quantization.quantize_dynamic(
    model_fp32, {torch.nn.Linear}, dtype=torch.qint8
)

# 推理速度提升 2-4 倍,模型大小减少 4 倍
```

#### 2. ONNX 导出

```python
# 导出为 ONNX 格式
dummy_input = torch.randn(1, 4, 8, 8)
torch.onnx.export(model, dummy_input, "model.onnx")

# 使用 ONNX Runtime 推理
import onnxruntime
session = onnxruntime.InferenceSession("model.onnx")
```

#### 3. 树复用

```python
# 人机对战中复用 MCTS 树
agent = TreeSearchAgent(network, numSimulations=800)

while not game_over:
    move = agent.selectMove(gameState)
    gameState.applyMove(move)
    agent.advanceTree(move)  # 复用子树

# 首步后每步节省约 50% 搜索时间
```

---

